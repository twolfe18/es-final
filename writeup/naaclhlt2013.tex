\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}

\title{Final Project for Event Semantics}

\author{Travis Wolfe\\
	    Johns Hopkins University \\
	    {\tt travis@cs.jhu.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  I need to write an abstract.
\end{abstract}



\section{Introduction}
% 1 page
%%% GENERAL
- we want *some* account of learning
	- linguistic work we read takes "the lexicon" as given, and it's not
	- NLP is based on experts
	- people who learn a language do not need an expert, so *some* systems can do it without experts
	- whether or not humans come "preprogrammed" with the special sauce for learning language is an open question
	- but a system that can learn from nothing but bare data is highly desireable

- features suck
	- in NLP we have experts write down features that rarely come close to human performance
	- this mismatch means that there is something in our heads that didn't make it into the features
	- features fail to generalize: accross tasks, across languages
	- features are not egalitarian! high resource languages are the only ones that receive attention

- embeddings are somewhat neurally plausible
	- they are a simplification, but a much more natural fit than logic to the brain

%%% EVENTS
Turning more specifically to events and their descriptions,
we aim to utilize unsupervised feature learning in order to
come up with representations of events.
As is generally true with unsupervised learning, you cannot learn
things that are not present in the data, so this goal of learning event
representations comes hand in hand with a hypothesis about the
distribution of event descriptions and their arguments.
The hypothesis is that there is regularity in the realization of predicates
and their arguments which is independent of syntax which can be observed
in local fragments of text.
% cite chomsky as decrying "linear order" in text, and then brush it off
Previous work in learning features for words has operated in the
paradigm of language modeling, and the features they learn are
interesting but constrained (c.f. related work).
The main drawback is that these representations do not allow generalization
across syntactic category.
This ability is paramount to modeling events because they can be described
in a wide variety of syntactic realizations.
Take Figure \ref{eventDesc} for example, which gives three descriptions of a wedding,
and note that
1) there is no clear syntactic correspondence between the descriptions
and 2) a good amount of information is captured near the event's trigger word in each case.

% TODO some will quibble over the slight semantic differences in these examples
% rebut this by saying that while Jane being joyous is not the same as the wedding being
% 
\begin{figure*}[ht]
\begin{tabular}{ | l | l | l | l | }
\hline
phrase & event trigger category & ARG0 category & ARGM category \\
\hline
Jane's joyous wedding & NN & NNP+POS & JJ \\
Jane was joyous when she got married & VB & NNP+PRP & JJ \\
Jane joyously gave her vows & VBD+PRP\$+VBZ & NNP & RB \\
\hline
\end{tabular}
\caption{three syntactically disparate descriptions of the same event}
\label{eventDesc}
\end{figure*}


The claim is not that arguments are always close, but
that they are biased towards being close\footnote{Grice's Maxim of Manner suggests
that the speaker should be brief. If the speaker is to be both brief and informative,
related concepts must appear close together.}
and when they are not close, the intervening words are not
systematically idiosyncratic to the event's trigger word.

Thus we are beginning to describe a statistical theory of coherence
based on locality of predicate and argument trigger words.
What remains is to define mathematically how this coherence should be modelled.


\section{Model and Learning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% - rami's model as first stab as coherence
%	"before contiuing to syntax-free coherence, lets first discuss syntax dependent conherence
%	model that gives us a way to train on raw text
%	mention perturb
% - discuss syntactic problems, how we wont capture Figure 1
% - concept + syntax version of rami's model
%	revisit perturb


A primary goal of this work is to learn from unannotated text and
the simplest way to do this is via language modeling, i.e. inducing
a score or probability distribution over word sequences.
Previous work has shown that we can learn very fine grained information
by inducing features for a language model.
While previous work has referred to this as language modelling,
this has the unwanted connotation that we are interested in the
numerical scores that come out of the language model for some NLP task.
Instead, we'll refer to our modelling as addressing {\em coherence},
and we'll discuss the factors that lead to coherence and how we might learn them. 
Later \ref{section:unlearnability}, we'll discuss ways in which our model of coherence
fails as a general explanation of coherence and how we might improve it.

First, we will describe the model we are extending, that of \cite{rami}.
- low (64) dimensional vectors for every word (appearing at least 100 times)
% TODO


%we want each word to efficiently represent its
%syntactic properties because we believe that bits in our representation are scarce.
%This might be in terms of statistical estimation efficiency (more bits mean that we need more data to fit them), 
%or computational estimation efficiency (more bits means running our optimization proceedure will take longer).


\subsection{Nuts and Bolts}
To facilitate learning conceptual properties as distinct from syntactic ones,
we parameterize our model with two matrices, $W$ and $S$.
The rows of thse matrices will catpure the properties we want to learn.
Our model will define the vector for word $i$ with syntactic tag $t(i)$
as the concatenation of $W_{i,\bullet}$ and $S_{t(i),\bullet}$ and refer to it as $v_i$.
The syntactic tag information is what allows this model to separate the conceptual
and syntactic properties. For this work, we will use part of speect tags because
they are easy to aquire in a relatively low-resource setting \cite{pos},
but you could easily extend these tags to be more informative provided
the cardinality of the tagset is not too large or a provided a factorized representation is used.
% TODO are "conceptual" properties just residual of syntactic properties?

The model of coherence will take a phrase $x = [v_{i-k}, ..., v_i, ..., v_{i+k}]$,
concatenate all of the word representations into a very tall vector, and then score it
with a nonlinear function
\[
	s(x) = p^t \cdot \tanh( A \cdot x + b )
\]
where $b$ is a vector of offsets,
$\tanh$ operates element-wise over the affine transformation of the stacked phrase vector,
and $p$ projects the resulting vector down to a real value.
Scoring functions of this form are universal function approximators
provided $A$ is of sufficient dimension\cite{Hornik:1989}.

% perturb, margin
In order to train our coherence (language) model efficiently, we rely on
the discriminative training method proposed in \cite{rami} which is based
on random perturbations of observed data. The idea is you take a phrase $x$
and create a copy which is slighly corruped $\tilde{x}$. Given this pair,
we want to learn parameters so that $s(x) \ge s(\tilde{x}) + 1$.
The intuition is that our model will learn to score coherent phrases highly
and incoherent ones lowly.

The corruption model affects the parameters we learn.
We follow \cite{rami} in that we only corrupt the middle word of the language model,
but we still have to decide whether to corrupt the word, its part of speech tag, or both.
In this work we do not consider perturbations of both because that leads to a very
diffuse gradient which updates both the $W$ and $S$ matrices rather than just one.
As a heuristic for when to perturb the word vs it's tag, we use
\[
	p(\mbox{perturb word}) = \frac
		{\sqrt{|W|} \times H(p_W)}
	{\sqrt{|W|} \times H(p_W) + \sqrt{|S|} \times H(p_S)}
\]
where $|W|$ is the number of parameters in the $W$ matrix, $H(\cdot)$ is
the entropy of a distribution, and $p_W$ and $p_S$ are the unigram distributions
of words and part of speech tags respectively\footnote{You could consider methods
that make a perturbation choice based on the phrase, but these methods are
substantially slower at training time or would require a lot of offline computation.
A careful implementation could do this efficiently, but that is not in the scope of this paper.
The method used in this paper only requires a few bits of entropy and no memory accesses.}.



Given a large corpus, we first give every word that appears at least 100
times an index and label all other words as \texttt{OOV}\footnote{we will learn
one vector for \texttt{OOV} even though there are many \texttt{OOV} words}.
Numbers are replaced by a unique token that only says how many digits are in
the token\footnote{e.g. ``120" $\rightarrow$ \texttt{3D} and ``1995" $\rightarrow$ \texttt{4D}}.
We then break text up in windows of size 5 ($k=2$) and shuffle all of the instances.

Our dataset was the New York Times part of the Annotated Gigaword dataset \cite{agiga},
ranging from 2000 to 2004. This yields a dataset with 153,762 unique words and 
456,746,432 phrases.

The feedforward network was implemented in Theano \cite{theano},
a python library which provides automatic differentiation and support for GPU computing.
Training was run using a variant of batched stochastic gradient descent called AdaGrad \cite{adagrad}
\footnote{Batches of 500 examples worked best in our experiments}.









\section{Evaluation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
- pool all nomlex entries (which are words with POS)
	lookup their C+S representation
	for each pair (n,v):
		sort all items by $dist(c(n), \cdot)$
		compute rank of c(v) in this list
	show a few of these nearest neighbor lists (include syntax dependent versions too)

	compare this method to regular unsupervised + leave one out training of a non-linear projection of n->v

- compare learning curves for C+S model vs plain W model

- predict event type leave one out




\section{What we can't/won't learn (and why)} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{section:unlearnability}
% 1 page
- we are not modeling anything at the *instance* level, only type level
- we are not modeling the argument structure or theta roles of an event

- no instances: any time you see the word foo, it means exactly the same thing
	- forces instance-idiosyncratic information into the A matrix
	- A matrix is not specific to any setup (e.g. syntactic category), so this is a high burden
	- polysemy can be encoded in W, but there is no regular treatment of it, so training has to figure this out

- we take narrow windows: many arguments will be external to the window
	- even ARG0 won't consistenly be in the window of a predicate (mostly verb for this statement)
		because ARG0 may or may not be the syntactic subject
	- training proceedure never sees full views of entire argumen structure
	***	- e.g. if there is a negative interaction between arguments that don't appear together => can't learn this!


\section{Improvements}
- train on dependency parses (not low resource)
	- teaches you selectional preferences, but no better than the parser was!
- train on consecutive verbs or subjects
	- teaches you about discourse
	- the event described before and after an event is likely to be relevant (e.g. scripts)
- use a recurrent NN to get something like an HMM
	- an attempt to get over narrow windows
	- check, i think Mikolov did this
- use bags of words related to an event with an intruder


\section{Previous Work} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
- scalar adjectives (deMarneffe)
- relations on nominals (Mikolov)

compare to Mikolov CBOW and other bag of words methods and say that
what they get at is the same thing:
removing order means that they are really working with conceptual coherence
(and removing a lot of the syntactic requirements for coherence)
BUT, not *all* information in the syntax/order is irrelevant!
subj is usually ARG0
arg swapping "man bites dog" vs "dog bites man"
scope "every boy loves some girl" vs "some boy loves every girl"
prepositions "gave him the ball" == "gave the ball to him" (BoW gets this actually)
noun-noun compounds "foutain pen" vs "pen foutain"
how to learn passive alternation? "john killed" vs "john was killed"

\section{Conclusion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO
write this



\bibliography{sources}{}
\bibliographystyle{plain}

\end{document}
