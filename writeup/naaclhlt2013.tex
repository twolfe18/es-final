\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}

\title{Final Project for Event Semantics}

\author{Travis Wolfe\\
	    Johns Hopkins University \\
	    {\tt travis@cs.jhu.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  I need to write an abstract.
\end{abstract}



\section{Introduction}
% 1 page
%%% GENERAL
- we want *some* account of learning
	- linguistic work we read takes "the lexicon" as given, and it's not
	- NLP is based on experts
	- people who learn a language do not need an expert, so *some* systems can do it without experts
	- whether or not humans come "preprogrammed" with the special sauce for learning language is an open question
	- but a system that can learn from nothing but bare data is highly desireable

- features suck
	- in NLP we have experts write down features that rarely come close to human performance
	- this mismatch means that there is something in our heads that didn't make it into the features
	- features fail to generalize: accross tasks, across languages
	- features are not egalitarian! high resource languages are the only ones that receive attention

- embeddings are somewhat neurally plausible
	- they are a simplification, but a much more natural fit than logic to the brain

%%% EVENTS
Turning more specifically to events and their descriptions,
we aim to utilize unsupervised feature learning in order to
come up with representations of events.
As is generally true with unsupervised learning, you cannot learn
things that are not present in the data, so this goal of learning event
representations comes hand in hand with a hypothesis about the
distribution of event descriptions and their arguments.
The hypothesis is that there is regularity in the realization of predicates
and their arguments which is independent of syntax which can be observed
in local fragments of text.
% cite chomsky as decrying "linear order" in text, and then brush it off
Previous work in learning features for words has operated in the
paradigm of language modeling, and the features they learn are
interesting but constrained (c.f. related work).
The main drawback is that these representations do not allow generalization
across syntactic category.
This ability is paramount to modeling events because they can be described
in a wide variety of syntactic realizations.
Take Figure \ref{eventDesc} for example, which gives three descriptions of a wedding,
and note that
1) there is no clear syntactic correspondence between the descriptions
and 2) a good amount of information is captured near the event's trigger word in each case.

\begin{figure*}[ht]
\begin{tabular}{ | l | l | l | l | }
\hline
phrase & event trigger category & ARG0 category & ARGM category \\
\hline
Jane's joyous wedding ... & N & NN\$ & ADJ \\
Everyone was joyous when Jane got married & V & NN & ADJ \\
Jane joyously gave her vows & V+N & NN & ADV \\
\hline
\end{tabular}
\caption{three syntactically disparate descriptions of the same event}
\label{eventDesc}
\end{figure*}


The claim is not that arguments are always close, but
that they are biased towards being close\footnote{Grice's Maxim of Manner suggests
that the speaker should be brief. If the speaker is to be both brief and informative,
related concepts must appear close together.}
and when they are not close, the intervening words are not
systematically idiosyncratic to the event's trigger word.

Thus we are beginning to describe a statistical theory of coherence
based on locality of predicate and argument trigger words.
What remains is to define mathematically how this coherence should be modelled.

% don't need to describe 1) below, 2) more naturally falls out of this setup

% separate ideas to add
% "I believe there is regularity in how arguments are mentioned near an event trigger"
% though, event triggers can be nominal or verbal
% since NNLM is overly sensitive to syntax, i want to capture the regularity in event triggers
% two ways to do it:
% 1) learn (non-)linear projection from nom(event) => verb(event)
% 2) model events and have nom(event) = event + nom, verb(event) = event + verb
% i opted for 2. the params learned for nom and verb are nonlinear in the score function

\section{Model and Learning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1/2 page
A primary goal of this work is to learn from unannotated text and
the simplest way to do this is via language modeling, i.e. inducing
a score or probability distribution over word sequences.
Previous work has shown that we can learn very fine grained information
by inducing features for a language model.
While previous work has referred to this as language modelling,
this has the unwanted connotation that we are interested in the
numerical scores that come out of the language model for some NLP task.
Instead, we'll refer to our modelling as addressing {\em coherence},
and we'll discuss the factors that lead to coherence and how we might learn them. 
Later \ref{section:unlearnability}, we'll discuss ways in which our model of coherence
fails as a general explanation of coherence and how we might improve it.

First, we will describe the model we are extending, that of \cite{rami}.
- low (64) dimensional vectors for every word (appearing at least 100 times)
% TODO

% this likely has to go
% it seems that I'm fitting my argument to the vectors not the other way around!
% the idea of softening formal semantics for a "dumb" learner has promise, but you need real examples
\subsection{Concepts vs Semantics}
Given a model of coherence, we wish to untie the model's notion of syntactic
and semantic, or perhaps more appriately, {\em conceptual} coherence.
From now on we will talk about the concept of a word rather than its ``semantics"
for a few reasons. First, the properties of a word that we want to refer to do
not constitute a semantic representation in any formal sense, but rather encode
properties of a ``concept" that may be useful in a formal semantics.
For example, if we wanted to say that ``wedding" and ``marry" are conceptually
similar, because for instance, they can be used when describing the same event,
we would not want to constrain ourselves any formal semantic expression between
the two words. The urge to refine ``concepts" into pieces of a formal semantic theory
are appealing to a theoretician (for example for falsifiability purposes), but are
dispreferred from a learning perspective because they might place an unreasonably high
burden on the statistical learner\footnote{the analogy to teaching a human might be that
if you are teaching a student physics, it is best to start with the theory of
Newtonian physics and work your way up to General Relativity rather than insist from
day 1 that space-time is non-Euclidean}.
For example we might also want to argue that the concepts for ``own" and ``purchase"
are related because for someone to own a thing the almost certainly had to purchase it.
This is different from the relationship between ``wedding" and ``marry", but a notion
of similarity that ties both of these pairs together can be useful in explaining
event coreference.

\subsection{Untying Conceptual and Syntactic Coherence}
The training regime will mean that our model must be able to distinguish
between syntactically malformed phrases from correct ones,
so we cannot ignore modeling syntax.
However, on the assumption that we want to have a representation
for every word, we want each word to efficiently represent its
syntactic properties because we believe that bits in our representation are scarce.
This might be in terms of statistical estimation efficiency (more bits mean that we need more data to fit them), 
or computational estimation efficiency (more bits means running our optimization proceedure will take longer).


% I'm going to need to revist the proportions of attention i want to spend
% and how i'm going to weave together the very general with the implementation
% details, but I should get down some implementation details.
\subsection{Nuts and Bolts}
% mention NOMLEX!
To facilitate learning conceptual properties as distinct from syntactic ones,
we parameterize our model with two matrices, $W$ and $S$.
The rows of thse matrices will catpure the properties we want to learn.
Our model will define the vector for word $i$ with syntactic tag $t(i)$
as the concatenation of $W_{i,\bullet}$ and $S_{t(i),\bullet}$ and refer to it as $v_i$.
The syntactic tag information is what allows this model to separate the conceptual
and syntactic properties. For this work, we will use part of speect tags because
they are easy to aquire in a relatively low-resource setting \cite{pos},
but you could easily extend these tags to be more informative provided
the cardinality of the tagset is not too large or a provided a factorized representation is used.
% TODO are "conceptual" properties just residual of syntactic properties?

The model of coherence will take a phrase $x = [v_{i-k}, ..., v_i, ..., v_{i+k}]$,
concatenate all of the word representations into a very tall vector, and then score it
with a nonlinear function
\[
	s(x) = p^t \cdot \tanh( A \cdot x + b )
\]
where $b$ is a vector of offsets,
$\tanh$ operates element-wise over the affine transformation of the stacked phrase vector,
and $p$ projects the resulting vector down to a real value.
Scoring functions of this form are universal function approximators
provided $A$ is of sufficient dimension\cite{Hornik:1989}.

% perturb, margin
In order to train our coherence (language) model efficiently, we rely on
the discriminative training method proposed in \cite{rami} which is based
on random perturbations of observed data. The idea is you take a phrase $x$
and create a copy which is slighly corruped $\tilde{x}$. Given this pair,
we want to learn parameters so that $s(x) \ge s(\tilde{x}) + 1$.
The intuition is that our model will learn to score coherent phrases highly
and incoherent ones lowly.

The corruption model affects the parameters we learn.
We follow \cite{rami} in that we only corrupt the middle word of the language model,
but we still have to decide whether to corrupt the word, its part of speech tag, or both.
In this work we do not consider perturbations of both because that leads to a very
diffuse gradient which updates both the $W$ and $S$ matrices rather than just one.
As a heuristic for when to perturb the word vs it's tag, we use
\[
	p(\mbox{perturb word}) = \frac
		{\sqrt{|W|} \times H(p_W)}
	{\sqrt{|W|} \times H(p_W) + \sqrt{|S|} \times H(p_S)}
\]
where $|W|$ is the number of parameters in the $W$ matrix, $H(\cdot)$ is
the entropy of a distribution, and $p_W$ and $p_S$ are the unigram distributions
of words and part of speech tags respectively\footnote{You could consider methods
that make a perturbation choice based on the phrase, but these methods are
substantially slower at training time or would require a lot of offline computation.
A careful implementation could do this efficiently, but that is not in the scope of this paper.
The method used in this paper only requires a few bits of entropy and no memory accesses.}.



Given a large corpus, we first give every word that appears at least 100
times an index and label all other words as \texttt{OOV}\footnote{we will learn
one vector for \texttt{OOV} even though there are many \texttt{OOV} words}.
Numbers are replaced by a unique token that only says how many digits are in
the token\footnote{e.g. ``120" $\rightarrow$ \texttt{3D} and ``1995" $\rightarrow$ \texttt{4D}}.
We then break text up in windows of size 5 ($k=2$) and shuffle all of the instances.

Our dataset was the New York Times part of the Annotated Gigaword dataset \cite{agiga},
ranging from 2000 to 2004. This yields a dataset with 153,762 unique words and 
456,746,432 phrases.

The feedforward network was implemented in Theano \cite{theano},
a python library which provides automatic differentiation and support for GPU computing.
Training was run using a variant of batched stochastic gradient descent called AdaGrad \cite{adagrad}
\footnote{Batches of 500 examples worked best in our experiments}.






\section{Previous Work} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1/2 page
% TODO maybe put this at the end?





\section{Results} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1/2 page





\section{What we can learn (and why)} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1 page
- information in distant words is tied
	i,j,k : same-window(i,j) and same-window(j,k) => tied(i,k)
	not clear how strong this is
- scalar adjectives (deMarneffe)
- relations on nominals (Mikolov)

- if windows are wide enough and we have enough bits
	- then human's solution is feasible
	- what i mean is that we are trying a task that is not ill-posed
		- like "predict tomorrows stock prices from last year's weather forecasts"

- parsing is possible in a neural net
	- just need a series of maxs, just like CRF parsing


\section{What we can't/won't learn (and why)} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{section:unlearnability}
% 1 page
- we are not modeling anything at the *instance* level, only type level
- we are not modeling the argument structure or theta roles of an event

- no instances: any time you see the word foo, it means exactly the same thing
	- forces instance-idiosyncratic information into the A matrix
	- A matrix is not specific to any setup (e.g. syntactic category), so this is a high burden
	- polysemy can be encoded in W, but there is no regular treatment of it, so training has to figure this out

- we take narrow windows: many arguments will be external to the window
	- even ARG0 won't consistenly be in the window of a predicate (mostly verb for this statement)
		because ARG0 may or may not be the syntactic subject
	- training proceedure never sees full views of entire argumen structure
	***	- e.g. if there is a negative interaction between arguments that don't appear together => can't learn this!


\section{Improvements}
- train on dependency parses (not low resource)
	- teaches you selectional preferences, but no better than the parser was!
- train on consecutive verbs or subjects
	- teaches you about discourse
	- the event described before and after an event is likely to be relevant (e.g. scripts)
- use a recurrent NN to get something like an HMM
	- an attempt to get over narrow windows
	- check, i think Mikolov did this
- use bags of words related to an event with an intruder




\bibliography{sources}{}
\bibliographystyle{plain}

\end{document}
