\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}

\title{Word Vector Representations for \\
Syntactically Disparate Event Descriptions}

\author{Travis Wolfe\\
	    Johns Hopkins University \\
	    {\tt travis@cs.jhu.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In this work we present an unsupervised method for
learning word vector representations that are conducive to
identifying coreferent mentions of events. This is a hard problem
because event mentions are weakly tied to syntax and
can be realized in may ways.
We discuss the justifications for a model that decomposes
coherence into syntactic and conceptual aspects and
provide a training proceedure for it.
An evaluation framework is provided but without results due
to difficulty with numerical optimization.
\end{abstract}



\section{Introduction}
% 1 page
%%% GENERAL
Knowledge representations for language has been given many different treatments.
Much linguistic work ignores the problem of learning, or sometimes even specifying
the representation of knowledge on which they study linguistic regularities.
In NLP representations are often comprised of ad hoc feature vectors which are
only available as long as there is an expert able to write features.
These features are often task specific and difficult to come up with.
Connectionists seek unification between the study of the neural representations
of humans with practical tasks in NLP.
This work most closesly follows the connectionist track in trying to learn
vector representations for words that allow us to make predictions about more
abstract concepts in linguistics, like events and entities.
%We are not concerned with absolute adherence to neural properties of people,
%but rather with proposing a representation that is plausibly similar to a neural one
%and with showing how you might learn it.

%%% EVENTS
We offer a simple model of words which decomposes into conceptual properties
of the predicates and arguments they evoke/describe and syntactic properties
of the words themselves.
As is generally true with unsupervised learning, you cannot learn
things that are not present in the data, so this goal of learning event
representations comes hand in hand with a hypothesis about the
distribution of event descriptions and their arguments.
The hypothesis is that there is regularity in the realization of predicates
and their arguments which is independent of syntax which can be observed
in local fragments of text.
% cite chomsky as decrying "linear order" in text, and then brush it off
Previous work in learning features for words has operated in the
paradigm of language modeling, and the features they learn are
interesting but constrained (c.f. related work).
The main drawback is that these representations do not allow generalization
across syntactic category.
This ability is paramount to modeling events because they can be described
in a wide variety of syntactic realizations.
Take Figure \ref{eventDesc} for example, which gives three descriptions of a wedding,
and note that
1) there is no clear syntactic correspondence between the descriptions
and 2) a good amount of information is captured near the event's trigger word in each case.

% TODO some will quibble over the slight semantic differences in these examples
% rebut this by saying that while Jane being joyous is not the same as the wedding being
% 
\begin{figure*}[ht]
\begin{tabular}{ | l | l | l | l | }
\hline
phrase & event trigger category & ARG0 category & ARGM category \\
\hline
Jane's joyous wedding & NN & NNP+POS & JJ \\
Jane was joyous when she got married & VB & NNP+PRP & JJ \\
Jane joyously gave her vows & VBD+PRP\$+VBZ & NNP & RB \\
\hline
\end{tabular}
\caption{three syntactically disparate descriptions of the same event}
\label{eventDesc}
\end{figure*}


The claim is not that arguments are always close, but
that they are biased towards being close\footnote{Grice's Maxim of Manner suggests
that the speaker should be brief. If the speaker is to be both brief and informative,
related concepts must appear close together.}
and when they are not close, the intervening words are not
systematically idiosyncratic to the event/predicate being described.

Thus we are beginning to describe a statistical theory of coherence
based on locality of predicate and argument trigger words.
What remains is to define mathematically how this coherence should be modelled.


\section{Model} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% - rami's model as first stab as coherence
%	"before contiuing to syntax-free coherence, lets first discuss syntax dependent conherence
%	model that gives us a way to train on raw text
%	mention perturb
% - discuss syntactic problems, how we wont capture Figure 1
% - concept + syntax version of rami's model
%	revisit perturb

A primary goal of this work is to learn from unannotated text and
the simplest way to do this is via language modeling, i.e. inducing
a score or probability distribution over word sequences.
Previous work has shown that we can learn very fine grained information
by inducing features for a language model (c.f. related work).
While previous work has referred to this as language modelling,
this has the unwanted connotation that we are interested in the
numerical scores that come out of the language model for some NLP task.
Instead, we'll refer to our modelling as addressing {\em coherence},
and we'll discuss the factors that lead to coherence and how we might learn them. 
Later \ref{section:unlearnability}, we'll discuss ways in which our model of coherence
fails as a general explanation of coherence and how we might improve it.

Lets start with a simple model of coherence where we look at a small phrase of words
and decide if it is coherent. Since we don't know, or rather want to learn, how words
collocate and compose, we will impose very few restrictions on how the words might
interact in determining coherence. Lets use a single hidden layer neural network to
describe this process of determining coherence. The inputs to the network will be one
vector for every word in the phrase and the output will be a coherence score
(which we will refer to as $s(\cdot)$).
The hidden layer in the middle will store some notion of non-linear composition,
but we will not assume we know how to parse.

This network has the nice property that we can propagate a gradient all the
way from the output score back to the values in the word vectors.
We can then learn features for words by training the network to accomplish
a disciminative task that looks like comprehension.
Given a phrase $x$ and a slightly corrupted (and hence incoherent) phrase $\tilde{x}$,
we want to train our model to recognize that the score of the uncorrupted prhase
is more coherent than the corrupted one: $s(x) \ge s(\tilde{x}) + 1$.
This training proceedure, provided we can get coherent phrases and an automatic method of
corruption, means that we can train on as much unlabelled training data as we have.

The model just described closely resembles the model of \cite{rami}, and
can learn fine grained (seemingly) semantic similarities like ``Apple" is
similar to ``Dell" if you take similarity to mean cosine distance of the
vectors learned for ``Apple" and ``Dell".
The problem with this method is that you will learn that ``marry" and
``wedding" are very dissimilar. The reason for this is that they are from
different syntactic categories and you would never use one as a direct replacement
for the other.
If we want a more general notion of similarity, or in this case type-level event
coreference\footnote{type-level event coreference in the sense that instances may
not be coreferent, the phrase ``John married Mary" describes a different event from
``John and Mary's wedding", but holding the all of the arguments constant, coreference holds.},
then we need to relax the assumption of replacability in a sequence of words with
a less syntax dependent notion of coherence.

One way to do this is to remove the order of the words in the sequence
(this is used in the CBOW method of \cite{DBLP:journals/corr/abs-1301-3781}),
but this throws the baby out with the bathwater. With no order you cannot
tell the differnce between ``dog bites man" and ``man bites dog", and surely
you would learn a different conception of ``biting" if you thought people went
around doing it to dogs.
A more general way to state this is that syntax plays a role in semantic interpretation,
and we should not remove it entirely from our model of coherence.

Another way is to consider that words that describe events that have
different syntactic category live in different places of the vector space
we are learning our representation in, but that there is some kind of mapping
between event descriptions. For example we might want to learn a transformation
on our vector space that takes the vector for ``marry" (in the verbal part
of the space) to a vector near ``marriage" (in the nominal part of the space).

% test on NOMLEX, performance is bad
We tested the plausibility of a model like this by trying to learn this type
of transformation on labelled pairs of nominal and verbal trigger words for events
like ``testament" and ``testify". This data was taken from NOMLEX \cite{nomlex},
which provides us with 3,135 noun-verb pairs.
We took the embeddings of \cite{rami} and tried to learn a linear map that minimized
the Euclidean distance between the vector representation for the noun and the
projected vector for the verb:
\[
	\min_P \sum_{(n,v) \in \mbox{\texttt{NOMLEX}}}
		|| x_n - P x_v ||_2^2
\]

When we compared the error of this model (on training data), it
was not substantially lower than the variance from the mean of the nominal vectors,
$\sum ||x_n - E[x_n]||_2^2$, which indicates that this model has no hope
of relating this data in an accurate way. Nonlinear transforms were no attempted
due to the size of the data and the apparent difficulty of the mapping.


% shouldn't work anyway
On second inpsection it doesn't seem like this method of projection would work for a few reasons.
First, since the word representations have not been preprocessed for polysemy, projection is much harder than
it would be if we assumed that we were projecting a centroid in some conceptual space\footnote{this
is assuming that the values in a word vector can represent either a point in space (a.k.a. a centroid)
or a distribution like one that you might get out of a feature vector in a log-linear model}.
If a word vector needs to store two meaning, e.g. ``testament" as in ``old" or as in what you do in court,
then the projection would need to preserve this polysemy, and in a way that represents the
syntactic category the word appears in, even if that notion might be null in some sense
(e.g. there is no verb that corresponds to the ``old testament" sense of ``testament" because it is not an event).
%also instance level info


We then might return to our simpler model of coherence,
but try to force the vectors we learn for words to be less syntax dependent.
%One reason benefit of this model is that it can solve the polysemy problem by putting more
%infromation in the hidden layer via a weak form of composition\footnote{you
%might imagine weights leading into the hidden layer implementing a (soft)max
%over possible compositions}. 
We can do this by separately modeling the syntactic and conceptual aspects separately.
Lets define a word vector $v_i$ to be a composition of a vector representing its
syntactic properties $s_i$ and its non-syntactic properties $w_i$, $v_i = s_i \oplus w_i$.
As before, we will build up a phrase vector $x$ by stacking the words in a phrase,
$x = [v_{i-k}, ..., v_i, ..., v_{i+k}]$, for some small context size $k$.
We want to come up with a way of learning $s_i$ in a way that will favor syntax and
be tied between various words (else we have something that is equivalent to the first model),
so we will represent $s_i$ as a row in a matrix $S_{t(i),\bullet}$,
where $t(i)$ is the part of speech tag of word $i$. In principle $t(i)$ could be more
fine grained than a part of speech tag, but it should be easily determined by surface
forms of the text so that it can be used on large unlabelled datasets.

With regard to how $\oplus$ should be implemented, we could use multiplication,
additions, or some other operator, but we favor vector stacking because it
implies that there are dimensions that account for syntactic versus semantic
coherence, a distinction which we intuitively believe to exist.
%First, vector stacking encodes the belief that syntax is a distinct property
%of words from their idiosyncratic meaning. % TODO support this
%Second, they are computationally more appealing because it 



\section{Learning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To train our model, we sought a large dataset that has many mentions of events,
so we opt for a corpus comprised of newswire text taken from the New York Times,
a part of the Annotated Gigaword dataset \cite{agiga}, ranging from 2000 to 2004.
This yields a dataset with 153,762 unique words and 456,746,432 phrases.
Given this corpus, we first give every word that appears at least 100
times an index and label all other words as \texttt{OOV}\footnote{we will learn
one vector for \texttt{OOV} even though there are many \texttt{OOV} words}.
Numbers are replaced by a unique token that only says how many digits are in
the token\footnote{e.g. ``120" $\rightarrow$ \texttt{3D} and ``1995" $\rightarrow$ \texttt{4D}}.
We then break text up in windows of size 5 ($k=2$) and shuffle all of the instances.


The corruption model affects the parameters we learn.
We follow \cite{rami} in that we only corrupt the middle word of the language model,
but we still have to decide whether to corrupt the word, its part of speech tag, or both.
In this work we do not consider perturbations of both because that leads to a very
diffuse gradient which updates both the $W$ and $S$ matrices rather than just one.
As a heuristic for when to perturb the word vs it's tag, we use
\[
	p(\mbox{perturb word}) = \frac
		{\sqrt{|W|} \times H(p_W)}
	{\sqrt{|W|} \times H(p_W) + \sqrt{|S|} \times H(p_S)}
\]
where $|W|$ is the number of parameters in the $W$ matrix, $H(\cdot)$ is
the entropy of a distribution, and $p_W$ and $p_S$ are the unigram distributions
of words and part of speech tags respectively\footnote{You could consider methods
that make a perturbation choice based on the phrase, but these methods are
substantially slower at training time or would require a lot of offline computation.
A careful implementation could do this efficiently, but that is not in the scope of this paper.
The method used in this paper only requires a few bits of entropy and no memory accesses.}.



The feedforward network was implemented in Theano \cite{theano},
a python library which provides automatic differentiation and support for GPU computing.
Training was run using a variant of batched stochastic gradient descent called AdaGrad \cite{adagrad}
\footnote{Batches of 500 examples worked best in our experiments}.









\section{Evaluation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
- pool all nomlex entries (which are words with POS)
	lookup their C+S representation
	for each pair (n,v):
		sort all items by $dist(c(n), \cdot)$
		compute rank of c(v) in this list
	show a few of these nearest neighbor lists (include syntax dependent versions too)

	compare this method to regular unsupervised + leave one out training of a non-linear projection of n->v

- compare learning curves for C+S model vs plain W model

- predict event type leave one out




\section{What we can't/won't learn (and why)} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{section:unlearnability}
% 1 page
- we are not modeling anything at the *instance* level, only type level
- we are not modeling the argument structure or theta roles of an event

- no instances: any time you see the word foo, it means exactly the same thing
	- forces instance-idiosyncratic information into the A matrix
	- A matrix is not specific to any setup (e.g. syntactic category), so this is a high burden
	- polysemy can be encoded in W, but there is no regular treatment of it, so training has to figure this out

- we take narrow windows: many arguments will be external to the window
	- even ARG0 won't consistenly be in the window of a predicate (mostly verb for this statement)
		because ARG0 may or may not be the syntactic subject
	- training proceedure never sees full views of entire argumen structure
	***	- e.g. if there is a negative interaction between arguments that don't appear together => can't learn this!


\section{Improvements}

- train on dependency parses (not low resource)
	- teaches you selectional preferences, but no better than the parser was!

- train on consecutive verbs or subjects
	- teaches you about discourse
	- the event described before and after an event is likely to be relevant (e.g. scripts)

- use bags of words related to an event with an intruder

- adaptive corruption (more plausible interlopers)



\section{Previous Work} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Work in neural representations and language models has a long history
\cite{foundation1,foundation2,foundation3} inter alia,
and has only recently regained a lot of attention.

% - scalar adjectives (deMarneffe)

% collobert and weston
\cite{DBLP:conf/icml/CollobertW08} built a multi-task NLP system based
on vector representation models which was trained to predict NER tags,
chunknings, and semantic role labelings. The SRL task is the most relevant
to this work, but their training proceedure was fully supervised and
is differs greatly from this work.

% turian
\cite{turian} showed that features learned by
a few vector representation models improved the state of the art in NER tagging and chunking.


\cite{MikolovYZ13} analyzed the representations learned using
a recurrent neural network language model described in \cite{DBLP:conf/interspeech/KombrinkMKB11}
which was initially created to aid in machine translation.
\cite{MikolovYZ13} however showed that the representations learned by the
language model were not only very good for that task, but also for the
task of analogy completion, which was one of the tasks in SemEval 2012 \cite{semeval2012}.

\cite{rami} presented a discriminatively trained feedforward neural network
which induces representations with similar properties to those learned using
a probabilistic language models, but with greatly accelerated training.
This model achieved state of the art part of speech tagging accuracy (or near it)
for many languages given only a very small labeled training set and the representations
learned on unlabelled data.
\cite{DBLP:journals/corr/abs-1301-3781} also focussed on efficient training and
proposed a similar model that sums rather than concatenates the input vectors,
and another that is just the reverse prediction task with the same network structure
(predict neighboring words from central word instead of central word from neighboring words).
The representations worked very well at analogies that represent specific lexical shifts
like pluralization (e.g. ``dog" : ``dogs" :: ``cat" : ``cats"), superlativization
(e.g. ``easy" : ``easiest" :: ``lucky" : ``luckiest"), and
capital cities (e.g. ``Athens" : ``Greece" :: ``Oslo" : ``Norway").
Their best model got 50.0\% accuracy on these tasks, but only one of the analogy types
reflected a change in syntactic category, which was between adjectives and adverbs.

%compare to Mikolov CBOW and other bag of words methods and say that
%what they get at is the same thing:
%removing order means that they are really working with conceptual coherence
%(and removing a lot of the syntactic requirements for coherence)
%BUT, not *all* information in the syntax/order is irrelevant!
%subj is usually ARG0
%arg swapping "man bites dog" vs "dog bites man"
%scope "every boy loves some girl" vs "some boy loves every girl"
%prepositions "gave him the ball" == "gave the ball to him" (BoW gets this actually)
%noun-noun compounds "foutain pen" vs "pen foutain"
%how to learn passive alternation? "john killed" vs "john was killed"

\section{Conclusion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO
write this



\bibliography{sources}{}
\bibliographystyle{plain}

\end{document}







